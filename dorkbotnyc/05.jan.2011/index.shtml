<!--#include virtual="../header"-->
<p><br>
<br>

<p>


what: dorkbot-nyc meeting<br>
where: Location One (Greene between Canal & Grand)<br>
when: Wednesday, 05 January, 2011 (!?!), 7-9pm<br>
$$$: $$$FREE$$$ (donations to Location One appreciated!)<br>
<p>
+++++++
<p>
The 01052011th dorkbot-nyc meeting took place at 7pm on Wednesday, January 5th, 2011 at 
<a href="http://www.location1.org/hour-directions" class="link">Location One</a> in SoHo.
<p>
The meeting was free and open to the public. PLEASE BROUGHT SNACKS TO SHARE!!! WE WERE HUNGRY!!!
THERE WERE HOMEMADE CUPCAKES!!!
<p>
+++++++

<p>
It featured technologically mediated dietary differences of: 
<blockquote>
<table>

<tr><td>
<img src="hayes.jpg" border="0" width="100" align="left" hspace="15"></td>
<td>
<br>
Ted Hayes: The Dawn Chorus: Language, Emergence and Art<br>

Much has been made of the idea of emergence in generative art and
science -- software swarms, autonomous robotics, learning machines. But
could emergence play a role in one of the most mysterious and yet most
pervasive of all human behaviors -- language? Could machines not just speak
to each other, but actually emergently construct their own language?
When I was a kid, I could think of few more intriguing challenges than
to invent a language of my own -- and having experimented with that, I
turned later to inventing language-inventing machines. With this goal in
mind, I devised The Dawn Chorus: a group of electronic sculptures that
are able to sense their environment and speak, and through their
interactions, emergently develop a language with which to share their
experiences. Utilizing a microcontroller, radio module and an artificial
neural network I designed for the Arduino, these entities autonomously
establish conventional signifiers for the stimuli in their environments,
associating their experiences with short patterns of sound -- or as we
normally say, words. And listening to them, we can hear the echos of the
births of our own, human, languages.<br>
<a href="http://www.epiphanus.net" class="link">
http://www.epiphanus.net</a>


</td></tr>


<tr><td>
<img src="grant.jpg" border="0" width="100" align="left" hspace="15"></td>
<td>
<br>
Sarah and Lara Grant: FSP<br>
Felted Signal Processing is the electronic textiles lab for sisters Sarah and Lara, creating soft controllers, sensors and explorations in interface design, often (but not always) for sound generating hardware. It is also a synaesthetic exploration, an effort to make the action of shaping and playing with sound more tangible and germane to the sensual properties of sound itself.<br>
<a href="http://fsp.fm" class="link">http://fsp.fm</a>
<br><br>
</td></tr>

<tr><td>
<img src="torinomargolis" border="0" width="100" align="left" hspace="15"></td>
<td>
<br>

<p>
Torino:Margolis: FreEMG<br>
It was about one year ago that Torino:Margolis presented the new media/dance piece Action Potential at Dorkbot, which utilized prohibitively expensive, commercial Electromyography (EMG). EMG is a biomedical tool used to sense the electrical potential generated during muscle contraction and in Action Potential they used the EMG signal generated by the performer's movement and turned it into sound. An attendee at this particular Dorkbot meeting suggested we build them ourselves. Yes, we had thought about it, but were daunted by the task. The good news is that with the help of the electronics community we have succeeded in building our own EMG, a FreEMG, and will share how we did it with you!<br>
<a href="http://www.torinomargolis.com" class="link">
http://www.torinomargolis.com</a>

</td></tr>

</table> </blockquote>
<br><br>

Douglas's <a href="images" class="link">pics</a> from the meeting<br>
Roberto Tobar's <a href="tobar_images" class="link">pics</a> from the meeting
<p>
NEXT MEETING: 02 February 2011

<!--#include virtual="../footer"-->

